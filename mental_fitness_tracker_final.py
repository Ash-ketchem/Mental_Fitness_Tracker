# -*- coding: utf-8 -*-
"""Mental Fitness Tracker Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oO-VOulNKABAurEXlMdBUbsMhoKN-7E9
"""

pip install pandas numpy matplotlib seaborn scikit-learn

import numpy as np
import pandas as pd
import os

INPUT_DIR = "Dataset"
if not os.path.exists(INPUT_DIR): os.mkdir(INPUT_DIR)
# upload the csv files to this directory

print("files used are")
for file in os.listdir("Dataset"):
  print(file)

import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px

# dataset walkthrough

dataset = [pd.read_csv(os.path.join(INPUT_DIR,file)) for file in os.listdir(INPUT_DIR) if ".csv" in file]

for df in dataset:
  print(df.head())

# filling missing values

for df in dataset:
  numeric_columns = df.select_dtypes(include=[np.number]).columns
  df[numeric_columns] = df[numeric_columns].fillna(df[numeric_columns].mean())

# merging dataframes

merged_df = pd.merge(*dataset, on=["Entity", "Code", "Year"])
merged_df.head()

# preprocessing of the df

# convert data types
for col in [col for col in merged_df.columns if col not in ["Entity", "Code", "Year"]] :
  merged_df[col] = merged_df[col].astype(float)

# Renaming columns
merged_df.set_axis(['Country','Code','Year','mental_fitness', 'Schizophrenia', 'Bipolar_disorder', 'Eating_disorder','Anxiety','drug_usage','depression','alcohol'], axis='columns', inplace=True)
merged_df.head()

merged_df.isnull().sum()
merged_df.drop('Code', axis=1, inplace=True)
merged_df.size,merged_df.shape

# EXPLORATORY ANALYSIS

merged_df.info()

mean = merged_df['mental_fitness'].mean()
mean

#heatmap

plt.figure(figsize=(12,6))
sns.heatmap(merged_df.corr(), annot=True, cmap='viridis')
plt.plot()

sns.jointplot(merged_df, x="Schizophrenia", y="mental_fitness" ,kind="reg", color="m")
plt.show()

sns.jointplot(merged_df, x="depression", y="mental_fitness" ,kind="reg", color="m")
plt.show()

sns.jointplot(merged_df,x='Bipolar_disorder',y='mental_fitness',kind='reg',color='blue')
plt.show()

sns.pairplot(merged_df,corner=True)
plt.show()

fig = px.pie(merged_df, values='mental_fitness', names='Year', color_discrete_sequence=px.colors.qualitative.Safe)
fig.show()

fig=px.bar(merged_df.head(20), x='Year', y='mental_fitness',color='Year',template='plotly_dark',)
fig.show()

# YEARWISE VARIATIONS IN MENTAL FITNESS OF DIFFERENT COUNTRIES

fig = px.line(merged_df, x="Year", y="mental_fitness", color='Country',markers=True,color_discrete_sequence=['#1f77b4', '#ff7f0e'], template='plotly_dark')
fig.show()

# Training models

# transform categorical variables into numerical values
from sklearn.preprocessing import LabelEncoder
l=LabelEncoder()
for i in merged_df.columns:
    if merged_df[i].dtype == 'object':
        merged_df[i]=l.fit_transform(merged_df[i])

#THE  X AND y VARIABLES

X = merged_df.drop('mental_fitness', axis=1)
Y = merged_df['mental_fitness']

# TEST-TRAIN SPLIT
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

print("xtrain: ", X_train.shape)
print("xtest: ", X_test.shape)
print("ytrain: ", y_train.shape)
print("ytest: ", y_test.shape)

from sklearn.linear_model import  Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# creating a dictionary to strore the models and metrics

models = {
    "Linear Regression" : {
        "model" : LinearRegression,
        "performance":{}
    },
    "Random Forest" : {
        "model" : RandomForestRegressor,
        "performance":{}
    },
    "Gradient Boosting Regression" : {
        "model" : GradientBoostingRegressor,
        "performance":{}
    },
    "Lasso Regression" : {
        "model" : Lasso,
        "performance":{}
    }
}

# Train multiple models and store their performance metrics

# Iterate over each model in the 'models' dictionary
for key in models.keys():
    # Initialize the model
    model = models.get(key).get("model")()

    # Fit the model using the training data
    model.fit(X_train, y_train)

    # Evaluate model performance on the training set
    y_pred_train = model.predict(X_train)
    models.get(key)["performance"]["train"] = {
        "mse": mean_squared_error(y_train, y_pred_train),
        "rmse": np.sqrt(mean_squared_error(y_train, y_pred_train)),
        "r2": r2_score(y_train, y_pred_train)
    }

    # Evaluate model performance on the testing set
    y_pred_test = model.predict(X_test)
    models.get(key)["performance"]["test"] = {
        "mse": mean_squared_error(y_test, y_pred_test),
        "rmse": np.sqrt(mean_squared_error(y_test, y_pred_test)),
        "r2": r2_score(y_test, y_pred_test)
    }

# Sort models based on their performance metrics

models = dict(sorted(models.items(), key=lambda x: (
    x[1]["performance"]["test"]["mse"],          # Sort by mean squared error (lower is better)
    x[1]["performance"]["test"]["rmse"],         # Sort by root mean squared error (lower is better)
    -x[1]["performance"]["test"]["r2"]           # Sort by negative R-squared (higher is better)
)))

models

# evaluating the performance of models

for key, value in models.items():
    print(f"Model: {key}\n" + (len(key) + 10) * "-" + "\n")

    # Print model performance on the training set
    print("* Model performance on the training set\n")
    performance = value["performance"]["train"]
    print(f'\tMSE: {format(performance["mse"])}')
    print(f'\tRMSE: {format(performance["rmse"])}')
    print(f'\tR2 score: {format(performance["r2"])}\n')

    # Print model performance on the testing set
    print("* Model performance on the testing set\n")
    performance = value["performance"]["test"]
    print(f'\tMSE: {format(performance["mse"])}')
    print(f'\tRMSE: {format(performance["rmse"])}')
    print(f'\tR2 score: {format(performance["r2"])}\n')

import matplotlib.pyplot as plt

# Determine the number of rows and columns for the subplot grid
num_models = len(models.keys())
num_cols = 2
num_rows = (num_models + num_cols - 1) // num_cols

# Create the subplot grid
fig, axs = plt.subplots(num_rows, num_cols, figsize=(12, 6*num_rows))

# Flatten the axs array if there is only one row
if num_rows == 1:
    axs = [axs]

# Iterate over each model and subplot
for i, key in enumerate(models.keys()):
    # Creating an instance of the model
    model = models[key].get("model")()

    # Fit the model using the training data
    model.fit(X_train, y_train)

    # Predict on the testing set
    y_test_pred = model.predict(X_test)

    # Determine the subplot position
    row = i // num_cols
    col = i % num_cols

    # Plotting scatter plot of predicted vs. actual values
    axs[row][col].scatter(y_test, y_test_pred, label='Testing Set')

    # Add labels and title to the subplot
    axs[row][col].set_xlabel('Original Values')
    axs[row][col].set_ylabel('Predicted Values')
    axs[row][col].set_title(f'Scatter Plot of Actual vs. Predicted Values\nModel: {key}')

    # Add legend
    axs[row][col].legend()

# Remove any empty subplots if exists
if num_models % num_cols != 0:
    for j in range(num_models % num_cols, num_cols):
        fig.delaxes(axs[-1][j])

# Adjust spacing between subplots
plt.tight_layout()

# Display the plot
plt.show()

# getting the best performing and worst performing model

print(f"The best performing model is {next(iter(models.keys()))}")
print(f"The worst performing model is {next(reversed(models.keys()))}")

print("""
SUMMARY
-------

The report provides the performance metrics for four different models: Random Forest, Gradient Boosting Regression, Linear Regression, and Lasso Regression. Let's interpret the performance metrics:

1. Random Forest:
   - Training Set Performance:
     - MSE (Mean Squared Error): 0.000631760448966647
     - RMSE (Root Mean Squared Error): 0.025134845314157935
     - R2 Score: 0.9992788142433484
   - Testing Set Performance:
     - MSE: 0.003532375088835692
     - RMSE: 0.059433787434721774
     - R2 Score: 0.9959013724750254

   The Random Forest model shows excellent performance on both the training and testing sets. It achieves a very low MSE and RMSE, indicating a small difference between the predicted and actual values. The high R2 score (close to 1) suggests that the model explains a significant portion of the variance in the target variable.

2. Gradient Boosting Regression:
   - Training Set Performance:
     - MSE: 0.10020762301877463
     - RMSE: 0.3165558766138683
     - R2 Score: 0.8856080488304406
   - Testing Set Performance:
     - MSE: 0.1360309660135911
     - RMSE: 0.3688237600990358
     - R2 Score: 0.8421627806983663

   The Gradient Boosting Regression model performs well on both the training and testing sets. It achieves relatively low MSE and RMSE values, indicating reasonable accuracy. The R2 score suggests that the model explains a substantial portion of the variance, although not as high as the Random Forest model.

3. Linear Regression:
   - Training Set Performance:
     - MSE: 0.5727249398406692
     - RMSE: 0.756785927353746
     - R2 Score: 0.34620619292039345
   - Testing Set Performance:
     - MSE: 0.5971943940818214
     - RMSE: 0.7727835363682519
     - R2 Score: 0.30707319585614723

   The Linear Regression model has higher MSE and RMSE values compared to the previous models, indicating a larger difference between the predicted and actual values. The R2 score suggests that the model explains a relatively lower amount of variance in the target variable compared to the other models.

4. Lasso Regression:
   - Training Set Performance:
     - MSE: 0.874972822205833
     - RMSE: 0.935399819438636
     - R2 Score: 0.0011753064563894133
   - Testing Set Performance:
     - MSE: 0.863253251606427
     - RMSE: 0.9291142295791336
     - R2 Score: -0.0016358538028664604

   The Lasso Regression model performs the poorest among the four models. It has the highest MSE and RMSE values, indicating a larger difference between predicted and actual values. The negative R2 score suggests that the model doesn't explain the variance in the target variable well and performs worse than a baseline model.

In summary, based on the performance metrics, the Random Forest model shows the best performance, followed by the Gradient Boosting Regression model. The Linear Regression model performs moderately, while the Lasso Regression model performs the worst among the four models.
""")